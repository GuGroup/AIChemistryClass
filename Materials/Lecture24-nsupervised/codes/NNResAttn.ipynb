{"cells":[{"cell_type":"markdown","metadata":{"id":"1ACwXgb5bvmL"},"source":["# Mounting Google drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30550,"status":"ok","timestamp":1669356120360,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"},"user_tz":-540},"id":"jtM8rBP2a1uK","outputId":"e5900944-ded9-4ce3-cee6-e899d36f1230"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1262,"status":"ok","timestamp":1669356141310,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"},"user_tz":-540},"id":"bhEoBpj8a2zG","outputId":"257ffe78-a761-479a-a150-5ea97e92ff4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Class/AIChemistry/HandsOn14-Res,Attn\n"]}],"source":["cd drive/MyDrive/Class/AIChemistry/HandsOn14-Res,Attn"]},{"cell_type":"markdown","metadata":{"id":"XAizZzgTb0Bj"},"source":["#Data Loader"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"fh5RcfPlWmyv","executionInfo":{"status":"ok","timestamp":1669359295007,"user_tz":-540,"elapsed":2,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"}}},"outputs":[],"source":["import csv\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.dataloader import default_collate\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.nn as nn\n","\n","# https://www.kaggle.com/datasets/uciml/mushroom-classification\n","\n","def one_hot_encode(data,uniques):\n","    onehot = torch.zeros((len(data),len(uniques)))\n","    for i,d in enumerate(data):\n","        onehot[i,uniques.index(d)] = 1\n","    return onehot\n","\n","def normalize(d):\n","    d = np.array([float(dd) for dd in d])\n","    avg = np.mean(d)\n","    std = np.std(d)\n","    return torch.Tensor((d - avg)/std).reshape(-1,1), avg, std\n","\n","class Data(Dataset):\n","  def __init__(self, data_path):\n","        data = []\n","        with open(data_path) as f:\n","            reader = csv.reader(f)\n","            for i, row in enumerate(reader):\n","                if i == 0:\n","                    continue\n","                data.append(row)\n","        data = np.array(data)\n","        \n","        Ys_raw = list([float(f) for f in data[:,0]])\n","\n","        # extract X\n","        Xs1 = []\n","        Xs2 = []\n","        Xs3 = []\n","        Xs4 = []\n","        Xs5 = []\n","        Xs6 = []\n","        Xs7 = []\n","        Xs8 = []\n","        Xs9 = []\n","\n","        x_uniques = list(set(data[:,1]))\n","        x = one_hot_encode(data[:,1],x_uniques)\n","        Xs1.append(x)\n","        \n","        x_uniques = list(set(data[:,2]))\n","        x = one_hot_encode(data[:,2],x_uniques)\n","        Xs2.append(x)\n","        \n","        x, _, _ = normalize(data[:,3])\n","        Xs3.append(x)\n","        \n","        x_uniques = list(set(data[:,4]))\n","        x = one_hot_encode(data[:,4],x_uniques)\n","        Xs4.append(x)\n","        \n","        x, _, _ = normalize(data[:,5])\n","        Xs5.append(x)\n","\n","        x_uniques = list(set(data[:,6]))\n","        x = one_hot_encode(data[:,6],x_uniques)\n","        Xs6.append(x)        \n","\n","        x, _, _ = normalize(data[:,7])\n","        Xs7.append(x)\n","\n","        x, _, _ = normalize(data[:,8])\n","        Xs8.append(x)\n","\n","        x, _, _ = normalize(data[:,9])\n","        Xs9.append(x)\n","\n","        self.Xs1 = torch.cat(Xs1,axis=0)\n","        self.Xs2 = torch.cat(Xs2,axis=0)\n","        self.Xs3 = torch.cat(Xs3,axis=0)\n","        self.Xs4 = torch.cat(Xs4,axis=0)\n","        self.Xs5 = torch.cat(Xs5,axis=0)\n","        self.Xs6 = torch.cat(Xs6,axis=0)\n","        self.Xs7 = torch.cat(Xs7,axis=0)\n","        self.Xs8 = torch.cat(Xs8,axis=0)\n","        self.Xs9 = torch.cat(Xs9,axis=0)\n","        \n","        # save it to the object\n","        self.Ys_raw = torch.Tensor(Ys_raw)\n","        self.input_fea_len = (self.Xs1.shape[1],\n","                              self.Xs2.shape[1],\n","                              self.Xs3.shape[1],\n","                              self.Xs4.shape[1],\n","                              self.Xs5.shape[1],\n","                              self.Xs6.shape[1],\n","                              self.Xs7.shape[1],\n","                              self.Xs8.shape[1],\n","                              self.Xs9.shape[1])\n","  def normalize_y(self,idx):\n","    # extract car price\n","    Ys, avg, std = normalize(self.Ys_raw[idx])\n","    self.Ys = (self.Ys_raw-avg)/std\n","    self.Yavg = avg\n","    self.Ystd = std\n","  \n","  def denormalize_y(self,Ys):\n","    return np.array(Ys)*self.Ystd+self.Yavg\n","\n","  def __len__(self):\n","    return self.Ys_raw.shape[0]\n","\n","  def __getitem__(self, idx):\n","    y = self.Ys[idx]\n","    x = (self.Xs1[idx,:],self.Xs2[idx,:],self.Xs3[idx,:],self.Xs4[idx,:],self.Xs5[idx,:],self.Xs6[idx,:],self.Xs7[idx,:],self.Xs8[idx,:],self.Xs9[idx,:])\n","    return idx, y,x"]},{"cell_type":"markdown","metadata":{"id":"YZJI2AAob31p"},"source":["#Model"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"NTCcaMBOZJrb","executionInfo":{"status":"ok","timestamp":1669360452796,"user_tz":-540,"elapsed":2,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class NN(nn.Module):\n","    def __init__(self, input_fea_len, fea_len=64, n_layer=3):\n","        # model initialize. neural network layers\n","        super().__init__()\n","        \n","        self.act = nn.Softplus()\n","        self.lin1 = nn.Linear(input_fea_len,fea_len)\n","        self.lin2 = nn.Linear(fea_len,fea_len)\n","        self.lin3 = nn.Linear(fea_len,fea_len)\n","        self.output_layer = nn.Linear(fea_len, 1)\n","        \n","    def forward(self, x):\n","        x = self.lin1(x)\n","        x = self.act(x)\n","        x = self.lin2(x)\n","        x = self.act(x)\n","        x = self.lin3(x)\n","        x = self.act(x)\n","        x = self.output_layer(x)\n","        return x\n","\n","\n","\n","class NNResDense(nn.Module):\n","    def __init__(self, input_fea_len, fea_len=64, n_layer=3):\n","        # model initialize. neural network layers\n","        super().__init__()\n","        \n","        self.act = nn.Softplus()\n","        self.embedding = nn.Linear(input_fea_len,fea_len)\n","\n","        self.lin1 = nn.Linear(fea_len,fea_len)\n","        self.lin2 = nn.Linear(fea_len,fea_len)\n","        self.lin3 = nn.Linear(fea_len,fea_len)\n","\n","        self.output_layer = nn.Linear(fea_len, 1)\n","        \n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.act(x)\n","\n","        x1 = self.lin1(x)\n","        x1 = self.act(x1)\n","    \n","        x2 = self.lin2(x1)\n","        x2 = self.act(x2)\n","        \n","        x3 = self.lin3(x2+x1)\n","        x3 = self.act(x3)\n","\n","        z = self.act(x1+x2+x3)\n","        z = self.output_layer(z)\n","        return z\n","\n","\n","\n","class NNAttn(nn.Module):\n","    def __init__(self, input_fea_len, fea_len=32):\n","        # model initialize. neural network layers\n","        super().__init__()\n","        \n","        self.act = nn.Softplus()\n","        self.embedding = nn.ModuleList([nn.Linear(x,fea_len) for x in input_fea_len])\n","        nhead = 4\n","        ndes = len(input_fea_len)\n","        self.lin = nn.Linear(fea_len,3*nhead*fea_len)\n","        self.vsoftmax = nn.Softmax(dim=2)\n","\n","        self.output_layer = nn.Linear(fea_len*ndes*nhead, 1)\n","        self.nhead = nhead\n","        self.ndes = ndes\n","        self.fea_len = fea_len\n","    def forward(self, x):\n","        x = [self.embedding[i](xx) for i,xx in enumerate(x)]\n","        x = torch.stack(x,1)\n","        qkv = self.lin(x)\n","        q,k,v = torch.split(qkv,self.nhead*self.fea_len,dim=2)\n","        q = q.reshape(q.shape[0],self.ndes,self.nhead,self.fea_len) # b * q * h * c\n","        k = k.reshape(q.shape[0],self.ndes,self.nhead,self.fea_len) # b * v * h * c\n","        v = v.reshape(q.shape[0],self.ndes,self.nhead,self.fea_len) # b * v * h * c\n","        qk = torch.einsum('bqhc,bvhc->bqvh', q, k)\n","        attn = self.vsoftmax(qk)  # b * q * v * h\n","        out = torch.einsum('bqvh,bvhc->bqhc', attn, v) # b * q * h * c\n","        out = out.reshape(x.shape[0],-1)\n","        z = self.act(out)\n","        z = self.output_layer(z)\n","        return z"]},{"cell_type":"markdown","metadata":{"id":"dQzQHiMsb6ly"},"source":["# Using Model"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"URHJwy54YA6N","executionInfo":{"status":"ok","timestamp":1669359366838,"user_tz":-540,"elapsed":475,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"}}},"outputs":[],"source":["from time import time\n","import random\n","import numpy as np\n","import torch.optim as optim\n","\n","def use_model(data_loader, model, criterion, optimizer, i_iter, mode, name = None):\n","  assert mode in ['train','predict']\n","  #switch to model mode\n","  if mode == 'train':\n","    model.train()\n","  elif mode == 'predict':\n","    model.eval() # activates all neurons in the dropout layer\n","\n","  targets = []\n","  outputs = [] \n","  idxss=[]\n","  for idxs, ys,xs in data_loader: # loop for each batch\n","    # move input to cuda\n","    if next(model.parameters()).is_cuda:\n","      for i in range(len(xs)):\n","        xs[i] = xs[i].to(device='cuda')\n","      ys = ys.to(device='cuda')\n","        \n","    #compute output\n","    if mode == 'train':\n","      output = model(xs)\n","      outputs += output.detach().cpu().tolist()\n","    elif mode == 'predict':\n","      with torch.no_grad(): # it does not compute the gradient. so it's faster\n","        output = model(xs)\n","      outputs += output.cpu().tolist()\n","    # Measure accuracy\n","    ys = ys.reshape(-1,1)\n","    loss = criterion(output, ys)\n","    \n","    # Backward propagation\n","    if mode == 'train':\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","    \n","    targets += ys.cpu().tolist() # list concatenation\n","    idxss += idxs.tolist()\n","  return outputs,targets,idxss"]},{"cell_type":"markdown","metadata":{"id":"acZMDyqAcE8N"},"source":["# Model Training and Validation"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"DR0Crd-rcDuy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669360506836,"user_tz":-540,"elapsed":49898,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"}},"outputId":"e40e0bf6-a2c0-4b19-bed5-b287d981ebdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading data...completed 0.14378976821899414 sec\n","Train MSE loss [0]: tensor(0.5066)\n","Val MSE loss [0]: 0.2994920313358307 <-Best\n","Train MSE loss [1]: tensor(0.2229)\n","Val MSE loss [1]: 0.22565555572509766 <-Best\n","Train MSE loss [2]: tensor(0.1713)\n","Val MSE loss [2]: 0.16049855947494507 <-Best\n","Train MSE loss [3]: tensor(0.1094)\n","Val MSE loss [3]: 0.11831630766391754 <-Best\n","Train MSE loss [4]: tensor(0.0920)\n","Val MSE loss [4]: 0.08508443832397461 <-Best\n","Train MSE loss [5]: tensor(0.0825)\n","Val MSE loss [5]: 0.09104679524898529 \n","Train MSE loss [6]: tensor(0.0752)\n","Val MSE loss [6]: 0.08146826922893524 <-Best\n","Train MSE loss [7]: tensor(0.0710)\n","Val MSE loss [7]: 0.06368638575077057 <-Best\n","Train MSE loss [8]: tensor(0.0651)\n","Val MSE loss [8]: 0.05931686982512474 <-Best\n","Train MSE loss [9]: tensor(0.0714)\n","Val MSE loss [9]: 0.06413474678993225 \n","Train MSE loss [10]: tensor(0.0565)\n","Val MSE loss [10]: 0.054567284882068634 <-Best\n","Train MSE loss [11]: tensor(0.0577)\n","Val MSE loss [11]: 0.061726879328489304 \n","Train MSE loss [12]: tensor(0.0559)\n","Val MSE loss [12]: 0.0635228380560875 \n","Train MSE loss [13]: tensor(0.0532)\n","Val MSE loss [13]: 0.05781855061650276 \n","Train MSE loss [14]: tensor(0.0514)\n","Val MSE loss [14]: 0.053508639335632324 <-Best\n","Train MSE loss [15]: tensor(0.0530)\n","Val MSE loss [15]: 0.05217161029577255 <-Best\n","Train MSE loss [16]: tensor(0.0517)\n","Val MSE loss [16]: 0.07844816893339157 \n","Train MSE loss [17]: tensor(0.0508)\n","Val MSE loss [17]: 0.05876554548740387 \n","Train MSE loss [18]: tensor(0.0462)\n","Val MSE loss [18]: 0.05886632949113846 \n","Train MSE loss [19]: tensor(0.0468)\n","Val MSE loss [19]: 0.053591277450323105 \n","Train MSE loss [20]: tensor(0.0452)\n","Val MSE loss [20]: 0.05106464400887489 <-Best\n","Train MSE loss [21]: tensor(0.0535)\n","Val MSE loss [21]: 0.05883251503109932 \n","Train MSE loss [22]: tensor(0.0598)\n","Val MSE loss [22]: 0.056008484214544296 \n","Train MSE loss [23]: tensor(0.0423)\n","Val MSE loss [23]: 0.047122128307819366 <-Best\n","Train MSE loss [24]: tensor(0.0439)\n","Val MSE loss [24]: 0.05231895670294762 \n","Train MSE loss [25]: tensor(0.0434)\n","Val MSE loss [25]: 0.05038071423768997 \n","Train MSE loss [26]: tensor(0.0404)\n","Val MSE loss [26]: 0.0505528450012207 \n","Train MSE loss [27]: tensor(0.0443)\n","Val MSE loss [27]: 0.051885273307561874 \n","Train MSE loss [28]: tensor(0.0395)\n","Val MSE loss [28]: 0.05150599032640457 \n","Train MSE loss [29]: tensor(0.0443)\n","Val MSE loss [29]: 0.04974769055843353 \n","Train MSE loss [30]: tensor(0.0393)\n","Val MSE loss [30]: 0.05463205650448799 \n","Train MSE loss [31]: tensor(0.0418)\n","Val MSE loss [31]: 0.061042655259370804 \n","Train MSE loss [32]: tensor(0.0416)\n","Val MSE loss [32]: 0.05175631865859032 \n","Train MSE loss [33]: tensor(0.0418)\n","Val MSE loss [33]: 0.04654572904109955 <-Best\n","Train MSE loss [34]: tensor(0.0416)\n","Val MSE loss [34]: 0.05083039402961731 \n","Train MSE loss [35]: tensor(0.0403)\n","Val MSE loss [35]: 0.0674714669585228 \n","Train MSE loss [36]: tensor(0.0412)\n","Val MSE loss [36]: 0.05598624795675278 \n","Train MSE loss [37]: tensor(0.0416)\n","Val MSE loss [37]: 0.055833060294389725 \n","Train MSE loss [38]: tensor(0.0375)\n","Val MSE loss [38]: 0.04937056079506874 \n","Train MSE loss [39]: tensor(0.0359)\n","Val MSE loss [39]: 0.049195025116205215 \n","Train MSE loss [40]: tensor(0.0390)\n","Val MSE loss [40]: 0.0483425036072731 \n","Train MSE loss [41]: tensor(0.0366)\n","Val MSE loss [41]: 0.045736365020275116 <-Best\n","Train MSE loss [42]: tensor(0.0367)\n","Val MSE loss [42]: 0.06078878790140152 \n","Train MSE loss [43]: tensor(0.0359)\n","Val MSE loss [43]: 0.04622553288936615 \n","Train MSE loss [44]: tensor(0.0339)\n","Val MSE loss [44]: 0.048368316143751144 \n","Train MSE loss [45]: tensor(0.0388)\n","Val MSE loss [45]: 0.051109712570905685 \n","Train MSE loss [46]: tensor(0.0339)\n","Val MSE loss [46]: 0.050117067992687225 \n","Train MSE loss [47]: tensor(0.0328)\n","Val MSE loss [47]: 0.04585294425487518 \n","Train MSE loss [48]: tensor(0.0393)\n","Val MSE loss [48]: 0.05855092033743858 \n","Train MSE loss [49]: tensor(0.0346)\n","Val MSE loss [49]: 0.05123814195394516 \n","Train MSE loss [50]: tensor(0.0364)\n","Val MSE loss [50]: 0.04871511831879616 \n","Train MSE loss [51]: tensor(0.0330)\n","Val MSE loss [51]: 0.05285493656992912 \n","Train MSE loss [52]: tensor(0.0367)\n","Val MSE loss [52]: 0.04962805658578873 \n","Train MSE loss [53]: tensor(0.0392)\n","Val MSE loss [53]: 0.0495152585208416 \n","Train MSE loss [54]: tensor(0.0383)\n","Val MSE loss [54]: 0.05534086376428604 \n","Train MSE loss [55]: tensor(0.0330)\n","Val MSE loss [55]: 0.044500671327114105 <-Best\n","Train MSE loss [56]: tensor(0.0320)\n","Val MSE loss [56]: 0.04580218717455864 \n","Train MSE loss [57]: tensor(0.0348)\n","Val MSE loss [57]: 0.04588310793042183 \n","Train MSE loss [58]: tensor(0.0336)\n","Val MSE loss [58]: 0.05340331420302391 \n","Train MSE loss [59]: tensor(0.0347)\n","Val MSE loss [59]: 0.04889540374279022 \n","Train MSE loss [60]: tensor(0.0341)\n","Val MSE loss [60]: 0.04886412248015404 \n","Train MSE loss [61]: tensor(0.0294)\n","Val MSE loss [61]: 0.047142524272203445 \n","Train MSE loss [62]: tensor(0.0311)\n","Val MSE loss [62]: 0.049728695303201675 \n","Train MSE loss [63]: tensor(0.0315)\n","Val MSE loss [63]: 0.06279968470335007 \n","Train MSE loss [64]: tensor(0.0379)\n","Val MSE loss [64]: 0.0548894926905632 \n","Train MSE loss [65]: tensor(0.0313)\n","Val MSE loss [65]: 0.050314754247665405 \n","Train MSE loss [66]: tensor(0.0310)\n","Val MSE loss [66]: 0.04710786044597626 \n","Train MSE loss [67]: tensor(0.0324)\n","Val MSE loss [67]: 0.046918611973524094 \n","Train MSE loss [68]: tensor(0.0322)\n","Val MSE loss [68]: 0.04586450755596161 \n","Train MSE loss [69]: tensor(0.0316)\n","Val MSE loss [69]: 0.04716002568602562 \n","Train MSE loss [70]: tensor(0.0340)\n","Val MSE loss [70]: 0.044108547270298004 <-Best\n","Train MSE loss [71]: tensor(0.0288)\n","Val MSE loss [71]: 0.04505441337823868 \n","Train MSE loss [72]: tensor(0.0280)\n","Val MSE loss [72]: 0.04670654609799385 \n","Train MSE loss [73]: tensor(0.0294)\n","Val MSE loss [73]: 0.053348079323768616 \n","Train MSE loss [74]: tensor(0.0313)\n","Val MSE loss [74]: 0.0443037748336792 \n","Train MSE loss [75]: tensor(0.0293)\n","Val MSE loss [75]: 0.04520919919013977 \n","Train MSE loss [76]: tensor(0.0290)\n","Val MSE loss [76]: 0.043920617550611496 <-Best\n","Train MSE loss [77]: tensor(0.0292)\n","Val MSE loss [77]: 0.0424637570977211 <-Best\n","Train MSE loss [78]: tensor(0.0283)\n","Val MSE loss [78]: 0.04595308005809784 \n","Train MSE loss [79]: tensor(0.0313)\n","Val MSE loss [79]: 0.04717656224966049 \n","Train MSE loss [80]: tensor(0.0367)\n","Val MSE loss [80]: 0.056464407593011856 \n","Train MSE loss [81]: tensor(0.0293)\n","Val MSE loss [81]: 0.046923693269491196 \n","Train MSE loss [82]: tensor(0.0282)\n","Val MSE loss [82]: 0.0455319806933403 \n","Train MSE loss [83]: tensor(0.0281)\n","Val MSE loss [83]: 0.04545356333255768 \n","Train MSE loss [84]: tensor(0.0283)\n","Val MSE loss [84]: 0.05513410270214081 \n","Train MSE loss [85]: tensor(0.0277)\n","Val MSE loss [85]: 0.04667852818965912 \n","Train MSE loss [86]: tensor(0.0270)\n","Val MSE loss [86]: 0.050574593245983124 \n","Train MSE loss [87]: tensor(0.0273)\n","Val MSE loss [87]: 0.04853827506303787 \n","Train MSE loss [88]: tensor(0.0261)\n","Val MSE loss [88]: 0.048807013779878616 \n","Train MSE loss [89]: tensor(0.0298)\n","Val MSE loss [89]: 0.04788612946867943 \n","Train MSE loss [90]: tensor(0.0288)\n","Val MSE loss [90]: 0.051764197647571564 \n","Train MSE loss [91]: tensor(0.0319)\n","Val MSE loss [91]: 0.04675820469856262 \n","Train MSE loss [92]: tensor(0.0283)\n","Val MSE loss [92]: 0.07542742788791656 \n","Train MSE loss [93]: tensor(0.0294)\n","Val MSE loss [93]: 0.04716973751783371 \n","Train MSE loss [94]: tensor(0.0298)\n","Val MSE loss [94]: 0.04677745699882507 \n","Train MSE loss [95]: tensor(0.0269)\n","Val MSE loss [95]: 0.04824180528521538 \n","Train MSE loss [96]: tensor(0.0298)\n","Val MSE loss [96]: 0.043378543108701706 \n","Train MSE loss [97]: tensor(0.0290)\n","Val MSE loss [97]: 0.05846596509218216 \n","Train MSE loss [98]: tensor(0.0275)\n","Val MSE loss [98]: 0.04912758246064186 \n","Train MSE loss [99]: tensor(0.0263)\n","Val MSE loss [99]: 0.05358927696943283 \n"]}],"source":["def get_loader(dataset, collate_fn=default_collate,\n","                batch_size=64, idx_sets=None,\n","                num_workers=0, pin_memory=False):\n","  loaders = []\n","  for idx in idx_sets:\n","    loaders.append(DataLoader(dataset, batch_size=batch_size,\n","                      sampler=SubsetRandomSampler(idx),\n","                      num_workers=num_workers,\n","                      collate_fn=collate_fn, pin_memory=pin_memory))\n","  return loaders\n","from sklearn import metrics\n","################################ Input ####################################\n","# data\n","data_path='carprice.csv'\n","TrainValTeSplitst = [0.8, 0.1, 0.1]\n","\n","# Training\n","batch_size = 64\n","lr = 0.001\n","nepochs = 100\n","cuda = True\n","seed = 1234\n","###########################################################################\n","\n","# Loading data\n","print('loading data...',end=''); t = time()\n","data = Data(data_path)\n","print('completed', time()-t,'sec')\n","\n","# Make a split\n","## number of train and validation\n","ndata = len(data)\n","ntrain = int(ndata*TrainValTeSplitst[0])\n","nval = int(ndata*TrainValTeSplitst[1])\n","## randomize\n","idxs = list(range(ndata))\n","random.seed(seed)\n","random.shuffle(idxs)\n","## split index\n","train_idx = idxs[:ntrain]\n","val_idx = idxs[ntrain:ntrain+nval]\n","test_idx = idxs[ntrain+nval:]\n","\n","data.normalize_y(train_idx)\n","\n","## get data loader\n","train_loader, val_loader, test_loader = get_loader(data,\n","    batch_size=batch_size,idx_sets=[train_idx,val_idx,test_idx],pin_memory=True)\n","\n","#build model\n","model = NNAttn(data.input_fea_len)\n","if cuda:\n","  if torch.cuda.device_count() > 1:\n","    model = nn.DataParallel(model,device_ids=[0])\n","  model.cuda()\n","\n","## Training\n","criterion = torch.nn.MSELoss() # regression (continuous)\n","optimizer = optim.Adam(model.parameters(),lr,weight_decay= 0) # 0 means no penalty\n","\n","bestval_mseloss = float('inf')\n","for i_iter in range(nepochs): # epochs\n","  output,target,idxs = use_model(train_loader,model,criterion,optimizer,i_iter,'train') # training model\n","  print('Train MSE loss [%d]:'%i_iter, criterion(torch.Tensor(target), torch.Tensor(output)))\n","  output,target,idxs = use_model(val_loader,model,criterion,optimizer,i_iter,'predict','Val') # using the model to validation set\n","  Val_mseloss = criterion(torch.Tensor(target), torch.Tensor(output)).tolist() # validation MSE error is calculated\n","  print('Val MSE loss [%d]:'%i_iter, Val_mseloss, end=' ') \n","  if Val_mseloss < bestval_mseloss: # if validation set error is lower than previous best\n","    bestval_mseloss = Val_mseloss\n","    print('<-Best')\n","    torch.save(model.state_dict(),'Weights.pth.tar') # we save the data\n","  else: print('')\n","  #scheduler.step()\n","\n"]},{"cell_type":"code","source":["from tqdm import tqdm\n","print('Testing. Loading best model')\n","model.load_state_dict(torch.load('Weights.pth.tar'))\n","output,target,idxs = use_model(test_loader,model,criterion,optimizer,i_iter,'predict','Test')\n","I = np.argsort(idxs)\n","output = np.array(output)[I]\n","target = np.array(target)[I]\n","o, t = data.denormalize_y(output), data.denormalize_y(target)\n","print('Predict MeanAbsoluteError:', torch.mean(torch.abs(torch.Tensor(o)-torch.Tensor(t))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M2PxxwoLKRRv","executionInfo":{"status":"ok","timestamp":1669360512618,"user_tz":-540,"elapsed":450,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"}},"outputId":"c550598c-0c4e-4c8d-9169-1ac021ec2df1"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing. Loading best model\n","Predict MeanAbsoluteError: tensor(1995.3127)\n"]}]},{"cell_type":"code","source":["print(o[:5],t[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"al0V5auPZ7Am","executionInfo":{"status":"ok","timestamp":1669356578895,"user_tz":-540,"elapsed":705,"user":{"displayName":"구근호/에너지공학부","userId":"09292163357270917481"}},"outputId":"b37bdc07-2550-435c-ce44-2f7dd4dd5c7a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[22340.34386648]\n"," [37481.10502546]\n"," [18301.1342341 ]\n"," [29947.51594397]\n"," [18406.11547837]] [[20497.99960413]\n"," [37499.99973567]\n"," [18639.9994472 ]\n"," [29989.99941271]\n"," [20994.99969853]]\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMtrC/yEe4L2iRxiEKkgGFV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}