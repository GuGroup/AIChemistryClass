


from google.colab import drive
drive.mount('/content/drive')


cd drive/MyDrive/Class/AIChemistry/HandsOn10-Dropout





import csv
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.dataloader import default_collate
from torch.utils.data.sampler import SubsetRandomSampler
import torch.nn as nn

# https://www.kaggle.com/datasets/uciml/mushroom-classification

def one_hot_encode(data,uniques):
    onehot = torch.zeros((len(data),len(uniques)))
    for i,d in enumerate(data):
        onehot[i,uniques.index(d)] = 1
    return onehot

def normalize(d):
    d = np.array([float(dd) for dd in d])
    avg = np.mean(d)
    std = np.std(d)
    return torch.Tensor((d - avg)/std).reshape(-1,1), avg, std

class Data(Dataset):
  def __init__(self, data_path):
        data = []
        with open(data_path) as f:
            reader = csv.reader(f)
            for i, row in enumerate(reader):
                if i == 0:
                    continue
                data.append(row)
        data = np.array(data)
        
        Ys_raw = list([float(f) for f in data[:,0]])

        # extract X
        Xs = []

        x_uniques = list(set(data[:,1]))
        x = one_hot_encode(data[:,1],x_uniques)
        Xs.append(x)
        
        x_uniques = list(set(data[:,2]))
        x = one_hot_encode(data[:,2],x_uniques)
        Xs.append(x)
        
        x, _, _ = normalize(data[:,3])
        Xs.append(x)
        
        x_uniques = list(set(data[:,4]))
        x = one_hot_encode(data[:,4],x_uniques)
        Xs.append(x)
        
        x, _, _ = normalize(data[:,5])
        Xs.append(x)

        x_uniques = list(set(data[:,6]))
        x = one_hot_encode(data[:,6],x_uniques)
        Xs.append(x)        

        x, _, _ = normalize(data[:,7])
        Xs.append(x)

        x, _, _ = normalize(data[:,8])
        Xs.append(x)

        x, _, _ = normalize(data[:,9])
        Xs.append(x)

        Xs = torch.cat(Xs,axis=1)
        
        # save it to the object
        self.Ys_raw = torch.Tensor(Ys_raw)
        self.Xs = torch.Tensor(Xs)
        self.input_fea_len = Xs.shape[1]
  def normalize_y(self,idx):
    # extract car price
    Ys, avg, std = normalize(self.Ys_raw[idx])
    self.Ys = (self.Ys_raw-avg)/std
    self.Yavg = avg
    self.Ystd = std
  
  def denormalize_y(self,Ys):
    return np.array(Ys)*self.Ystd+self.Yavg

  def __len__(self):
    return self.Xs.shape[0]

  def __getitem__(self, idx):
    y = self.Ys[idx]
    x = self.Xs[idx,:]
    return idx, y,x





import torch.nn as nn

class NN(nn.Module):
    def __init__(self, input_fea_len, fea_len=64, n_layer=3):
        # model initialize. neural network layers
        super(NN, self).__init__()
        
        self.act = nn.Softplus()
        self.lin1 = nn.Linear(input_fea_len,fea_len)
        self.lin2 = nn.Linear(fea_len,fea_len)
        self.lin3 = nn.Linear(fea_len,fea_len)
        self.dropout_layer = torch.nn.Dropout(p=0.5)
        self.output_layer = nn.Linear(fea_len, 1)
        
    def forward(self, x):
        x = self.lin1(x)
        x = self.act(x)
        x = self.lin2(x)
        x = self.act(x)
        x = self.dropout_layer(x)
        x = self.lin3(x)
        x = self.act(x)
        x = self.dropout_layer(x)
        x = self.output_layer(x)
        return x





from time import time
import random
import numpy as np
import torch.optim as optim

def use_model(data_loader, model, criterion, optimizer, i_iter, mode, name = None):
  assert mode in ['train','predict']
  #switch to model mode
  if mode == 'train':
    model.train()
  elif mode == 'predict':
    model.eval() # activates all neurons in the dropout layer
    for l in model.modules():
      if l.__class__.__name__ == 'Dropout':
        l.train() # by doing this, we can continue random activation

  targets = []
  outputs = [] 
  idxss=[]
  for idxs, ys,xs in data_loader: # loop for each batch
    # move input to cuda
    if next(model.parameters()).is_cuda:
      xs = xs.to(device='cuda')
      ys = ys.to(device='cuda')
        
    #compute output
    if mode == 'train':
      output = model(xs)
      outputs += output.detach().cpu().tolist()
    elif mode == 'predict':
      with torch.no_grad(): # it does not compute the gradient. so it's faster
        output = model(xs)
      outputs += output.cpu().tolist()
    # Measure accuracy
    ys = ys.reshape(-1,1)
    loss = criterion(output, ys)
    
    # Backward propagation
    if mode == 'train':
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
    
    targets += ys.cpu().tolist() # list concatenation
    idxss += idxs.tolist()
  return outputs,targets,idxss





def get_loader(dataset, collate_fn=default_collate,
                batch_size=64, idx_sets=None,
                num_workers=0, pin_memory=False):
  loaders = []
  for idx in idx_sets:
    loaders.append(DataLoader(dataset, batch_size=batch_size,
                      sampler=SubsetRandomSampler(idx),
                      num_workers=num_workers,
                      collate_fn=collate_fn, pin_memory=pin_memory))
  return loaders
from sklearn import metrics
################################ Input ####################################
# data
data_path='carprice.csv'
TrainValTeSplitst = [0.8, 0.1, 0.1]

# Training
batch_size = 64
lr = 0.001
nepochs = 25
cuda = True
seed = 1234
###########################################################################

# Loading data
print('loading data...',end=''); t = time()
data = Data(data_path)
print('completed', time()-t,'sec')

# Make a split
## number of train and validation
ndata = len(data)
ntrain = int(ndata*TrainValTeSplitst[0])
nval = int(ndata*TrainValTeSplitst[1])
## randomize
idxs = list(range(ndata))
random.seed(seed)
random.shuffle(idxs)
## split index
train_idx = idxs[:ntrain]
val_idx = idxs[ntrain:ntrain+nval]
test_idx = idxs[ntrain+nval:]

data.normalize_y(train_idx)

## get data loader
train_loader, val_loader, test_loader = get_loader(data,
    batch_size=batch_size,idx_sets=[train_idx,val_idx,test_idx],pin_memory=True)

#build model
model = NN(data.input_fea_len)
if cuda:
  if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model,device_ids=[0])
  model.cuda()

## Training
criterion = torch.nn.MSELoss() # regression (continuous)
optimizer = optim.Adam(model.parameters(),lr,weight_decay= 0) # 0 means no penalty

bestval_mseloss = float('inf')
for i_iter in range(nepochs): # epochs
  output,target,idxs = use_model(train_loader,model,criterion,optimizer,i_iter,'train') # training model
  print('Train MSE loss [%d]:'%i_iter, criterion(torch.Tensor(target), torch.Tensor(output)))
  output,target,idxs = use_model(val_loader,model,criterion,optimizer,i_iter,'predict','Val') # using the model to validation set
  Val_mseloss = criterion(torch.Tensor(target), torch.Tensor(output)).tolist() # validation MSE error is calculated
  print('Val MSE loss [%d]:'%i_iter, Val_mseloss, end=' ') 
  if Val_mseloss < bestval_mseloss: # if validation set error is lower than previous best
    bestval_mseloss = Val_mseloss
    print('<-Best')
    torch.save(model.state_dict(),'Weights.pth.tar') # we save the data
  else: print('')
  #scheduler.step()




from tqdm import tqdm
print('Testing. Loading best model')
model.load_state_dict(torch.load('Weights.pth.tar'))
output_dist = []
for _ in tqdm(range(1000),total=1000):
  output,target,idxs = use_model(test_loader,model,criterion,optimizer,i_iter,'predict','Test')
  I = np.argsort(idxs)
  output = np.array(output)[I]
  target = np.array(target)[I]
  #print('Predict MSEloss score:', criterion(torch.Tensor(target), torch.Tensor(output)))
  o, t = data.denormalize_y(output)[:10], data.denormalize_y(target)[:10]
  output_dist.append(o)



output_dist = np.array(output_dist)
i = 0
import matplotlib.pyplot as plt
plt.hist(output_dist[:,i,0])
plt.plot([t[i],t[i]],[0,500])
std = np.std(output_dist[:,:,0],axis=0)
print(std)


for l in model.modules():
  if l.__class__.__name__ == 'Dropout':
    l.train()
